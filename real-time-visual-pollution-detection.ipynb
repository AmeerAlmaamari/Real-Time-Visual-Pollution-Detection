{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "t6MPjfT5NrKQ"
   },
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "  <a href=\"https://ultralytics.com/yolov8\" target=\"_blank\">\n",
    "    <img width=\"1024\", src=\"https://i.imgur.com/STDGXU4.jpg\"></a>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Lammah (Observer) is a state-of-the-art machine learning model for the detection and evaluation of visual pollution elements. The model is trained to accurately identify and locate these elements within any kind of devices in real-time, allowing for efficient and automated analysis of the level of visual pollution present in a given area. This powerful tool allows for effective planning and execution of clean-up efforts, as well as for monitoring the progress of such efforts over time, thus having a significant impact on the community's aesthetic and environment.\n",
    "\n",
    "The model is trained using YOLOv8 <a href=\"https://ultralytics.com\">Ultralytics</a>\n",
    "</div>\n",
    "\n",
    "Note: Please contact me if you need the weights of the models, they are not uploaded due to file's size\n",
    "\n",
    "Dataset link: https://drive.google.com/file/d/1ULqYtd9yomeGz53WBhgRdPRFB37ppeDU/view"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "7mGmQbAO5pQb"
   },
   "source": [
    "# 0. Setup\n",
    "\n",
    "Pip install `ultralytics` and [dependencies](https://github.com/ultralytics/ultralytics/blob/main/requirements.txt) and check PyTorch and GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "wbvMlHd_QwMG",
    "outputId": "5006941e-44ff-4e27-f53e-31bf87221334"
   },
   "outputs": [],
   "source": [
    "# Pip install method (recommended)\n",
    "%pip install ultralytics\n",
    "import ultralytics\n",
    "ultralytics.checks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TUFPge7f_1ms"
   },
   "outputs": [],
   "source": [
    "# Git clone method (for development)\n",
    "!git clone https://github.com/ultralytics/ultralytics\n",
    "%pip install -qe ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the needed libraries\n",
    "from imgaug import augmenters as iaa\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import shutil\n",
    "import cv2\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "4JnkELT0cIJg"
   },
   "source": [
    "# 1. Preprocessing\n",
    "<p align=\"\"><a><img width=\"1000\" src=\"https://i.imgur.com/2Z4n4n3.jpg\"/></a></p>\n",
    "This section contains the experiments of data preprocessing including:\n",
    "\n",
    "| Task   | Description   \n",
    "| -------- | ----------- | \n",
    "| Annotations Preparation | Convert images coordinates to Yolov8 annotations form |\n",
    "| Class-based augmentation | A proposed technique from me to augment the <br> images based on classes frequency in the dataset. |\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Annotations Preparation\n",
    "\n",
    "Validate a model's performance on the dataset's `val` or `test` splits (90% `training`, 10% `validation`). The best custom-trained model is used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zR9ZbuQCH7FX",
    "outputId": "3136de6b-2995-4731-e84c-962acb233d89"
   },
   "outputs": [],
   "source": [
    "# Load the data from the CSV file\n",
    "df = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create annotations folder\n",
    "if not os.path.exists(\"annotations\"):\n",
    "    os.makedirs(\"annotations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by the \"image_path\" column and count the number of rows for each value\n",
    "counts = df.groupby(\"image_path\").size()\n",
    "df[\"num_objects\"] = df[\"image_path\"].apply(lambda x: counts[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the annotations .txt files in COCO format\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    class_name = row['name']\n",
    "    image_path = row['image_path']\n",
    "    class_ = int(row['class'])\n",
    "\n",
    "    # Load the image\n",
    "    image = cv2.imread(os.path.join(\"images\", image_path))\n",
    "    height, width, _ = image.shape\n",
    "    xmin = max(0,min(int(row['xmin'])*2, width))\n",
    "    ymin = max(0,min(int(row['ymin'])*2, height))\n",
    "    xmax = max(0,min(int(row['xmax'])*2, width))\n",
    "    ymax = max(0,min(int(row['ymax'])*2, height))\n",
    "    \n",
    "    \n",
    "    b_width = abs(xmax-xmin)\n",
    "    b_heigth = abs(ymax-ymin)\n",
    "    b_center_x = xmin + (b_width)/2\n",
    "    b_center_y = ymin + (b_heigth)/2\n",
    "    \n",
    "    # Normalize the coordinates\n",
    "    center_x_norm = round((b_center_x / width), 8)\n",
    "    center_y_norm = round((b_center_y / height), 8)\n",
    "    b_width_norm = round((b_width / width), 8)\n",
    "    b_heigth_norm = round((b_heigth / height), 8)\n",
    "    \n",
    "    \n",
    "    # Open the text file for the image\n",
    "    with open(os.path.join(\"annotations\", image_path) + '.txt', 'a') as f:\n",
    "      # Write the information to the file\n",
    "      f.write(str(class_) + ' ' + str(center_x_norm) + ' ' + str(center_y_norm) + ' ' + str(b_width_norm) + ' ' + str(b_heigth_norm) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' assign variable names for the folders where:\n",
    "1- annotations: The folder where COCO format images are stored\n",
    "2- images: Where all of the images are stored\n",
    "3- train: Where train images will be stored\n",
    "4- valid: Where validation images will be stored\n",
    "5- test: Where test images will be stored\n",
    "'''\n",
    "annotations_dir = 'annotations'\n",
    "images_dir = 'images'\n",
    "train_images_dir = 'train'\n",
    "valid_images_dir = 'valid'\n",
    "test_images_dir = 'test'\n",
    "\n",
    "# create folders for the images and their labels (in COCO format)\n",
    "if not os.path.exists(train_images_dir):\n",
    "    os.makedirs(train_images_dir)\n",
    "if not os.path.exists(os.path.join(train_images_dir,\"labels\")):\n",
    "    os.makedirs(os.path.join(train_images_dir,\"labels\"))\n",
    "if not os.path.exists(os.path.join(train_images_dir,\"images\")):\n",
    "    os.makedirs(os.path.join(train_images_dir,\"images\"))\n",
    "\n",
    "if not os.path.exists(valid_images_dir):\n",
    "    os.makedirs(valid_images_dir)\n",
    "if not os.path.exists(os.path.join(valid_images_dir,\"labels\")):\n",
    "    os.makedirs(os.path.join(valid_images_dir,\"labels\"))\n",
    "if not os.path.exists(os.path.join(valid_images_dir,\"images\")):\n",
    "    os.makedirs(os.path.join(valid_images_dir,\"images\"))\n",
    "    \n",
    "if not os.path.exists(test_images_dir):\n",
    "    os.makedirs(test_images_dir)\n",
    "if not os.path.exists(os.path.join(test_images_dir,\"labels\")):\n",
    "    os.makedirs(os.path.join(test_images_dir,\"labels\"))\n",
    "if not os.path.exists(os.path.join(test_images_dir,\"images\")):\n",
    "    os.makedirs(os.path.join(test_images_dir,\"images\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of txt files\n",
    "txt_files = [file for file in os.listdir(annotations_dir) if file.endswith('.jpg.txt')]\n",
    "\n",
    "# Rename the txt files\n",
    "for txt_file in txt_files:\n",
    "    old_file_name = os.path.join(annotations_dir, txt_file)\n",
    "    new_file_name = os.path.join(annotations_dir, txt_file.replace('.jpg.txt', '.txt'))\n",
    "    os.rename(old_file_name, new_file_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Splitting The Data\n",
    "\n",
    "Split training, validation and testing ratios.\n",
    "- Training Pecentage: 80%\n",
    "- Validation Percentage: 19%\n",
    "- Testing Percentage: 1%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_img = len(txt_files)\n",
    "train_ratio = 0.8\n",
    "valid_ratio = 0.19\n",
    "test_ratio = 0.01\n",
    "train_idx = int(num_img*train_ratio)\n",
    "valid_idx = train_idx + int(num_img*valid_ratio)\n",
    "test_idx = valid_idx + int(num_img*test_ratio)\n",
    "\n",
    "train_images = txt_files[:train_idx]\n",
    "valid_images = txt_files[train_idx:valid_idx]\n",
    "test_images = txt_files[valid_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the corresponding images to the destination directory\n",
    "\n",
    "# Training images\n",
    "for txt_file in train_images:\n",
    "    image_name = txt_file.replace('.txt', '')\n",
    "    label_path = os.path.join(annotations_dir, image_name+\".txt\") \n",
    "    image_path = os.path.join(images_dir, image_name+\".jpg\")\n",
    "    \n",
    "    if os.path.exists(label_path):\n",
    "        shutil.copy(label_path, os.path.join(train_images_dir,\"labels\"))\n",
    "    \n",
    "    if os.path.exists(image_path):\n",
    "        shutil.copy(image_path, os.path.join(train_images_dir,\"images\"))\n",
    "\n",
    "        \n",
    "# Validation images\n",
    "for txt_file in valid_images:\n",
    "    image_name = txt_file.replace('.txt', '')\n",
    "    label_path = os.path.join(annotations_dir, image_name+\".txt\") \n",
    "    image_path = os.path.join(images_dir, image_name+\".jpg\")\n",
    "    \n",
    "    if os.path.exists(label_path):\n",
    "        shutil.copy(label_path, os.path.join(valid_images_dir,\"labels\"))\n",
    "    \n",
    "    if os.path.exists(image_path):\n",
    "        shutil.copy(image_path, os.path.join(valid_images_dir,\"images\"))\n",
    "        \n",
    "        \n",
    "# Test images\n",
    "for txt_file in test_images:\n",
    "    image_name = txt_file.replace('.txt', '')\n",
    "    label_path = os.path.join(annotations_dir, image_name+\".txt\") \n",
    "    image_path = os.path.join(images_dir, image_name+\".jpg\")\n",
    "    \n",
    "    if os.path.exists(label_path):\n",
    "        shutil.copy(label_path, os.path.join(test_images_dir,\"labels\"))\n",
    "    \n",
    "    if os.path.exists(image_path):\n",
    "        shutil.copy(image_path, os.path.join(test_images_dir,\"images\"))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Augmentation\n",
    "\n",
    "Augmentation is a crucial step in the preprocessing phase, as it creates and mimic the images to increase classes existance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths for the images and annotations\n",
    "images_path = \"train/images\"\n",
    "annotations_path = \"train/labels\"\n",
    "aug_images_path = images_path\n",
    "aug_annotations_path = annotations_path\n",
    "\n",
    "# Create a dictionary to store the class counts\n",
    "class_counts = defaultdict(int)\n",
    "class_names = {\n",
    "    0:\"GRAFFITI\",\n",
    "    1:\"FADED_SIGNAGE\",\n",
    "    2:\"POTHOLES\",\n",
    "    3:\"GARBAGE\",\n",
    "    4:\"CONSTRUCTION_ROAD\",\n",
    "    5:\"BROKEN_SIGNAGE\",\n",
    "    6:\"BAD_STREETLIGHT\",\n",
    "    7:\"BAD_BILLBOARD\",\n",
    "    8:\"SAND_ON_ROAD\",\n",
    "    9:\"CLUTTER_SIDEWALK\",\n",
    "    10:\"UNKEPT_FACADE\"\n",
    "}\n",
    "\n",
    "category_ids = class_names.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_bbox_coco2yolo(img_width, img_height, bbox, cls_):\n",
    "    \"\"\"\n",
    "    Convert bounding box from COCO  format to YOLO format\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img_width : int\n",
    "        width of image\n",
    "    img_height : int\n",
    "        height of image\n",
    "    bbox : list[int]\n",
    "        bounding box annotation in COCO format: \n",
    "        [top left x position, top left y position, width, height]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[float]\n",
    "        bounding box annotation in YOLO format: \n",
    "        [x_center_rel, y_center_rel, width_rel, height_rel]\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOLO bounding box format: [x_center, y_center, width, height]\n",
    "    # (float values relative to width and height of image)\n",
    "    converted_boxes = []\n",
    "    for bx, c in zip(bbox, cls_):\n",
    "        x_tl, y_tl, w, h = bx\n",
    "\n",
    "        dw = 1.0 / img_width\n",
    "        dh = 1.0 / img_height\n",
    "\n",
    "        x_center = x_tl + w / 2.0\n",
    "        y_center = y_tl + h / 2.0\n",
    "\n",
    "        x = x_center * dw\n",
    "        y = y_center * dh\n",
    "        w = w * dw\n",
    "        h = h * dh\n",
    "\n",
    "        converted_boxes.append([c, x, y, w, h])\n",
    "    return converted_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depending of the augmentation probability, get number of times to augment a certain image\n",
    "# the less the probability is, the more we generate augmented images\n",
    "def get_num_samples(proba):\n",
    "    if 0<=proba<=0.3:\n",
    "        return 4\n",
    "    if 0.3<proba<=0.5:\n",
    "        return 2\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in class_names.values():\n",
    "    class_counts[name]=0\n",
    "# Iterate over all of the annotations\n",
    "for annotation_file in os.listdir(annotations_path):\n",
    "    # Open the annotation file\n",
    "    with open(os.path.join(annotations_path, annotation_file), \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Iterate over each line of the annotation file\n",
    "    for line in lines:\n",
    "        # Split the line into parts\n",
    "        parts = line.strip().split()\n",
    "\n",
    "        # Get the class index\n",
    "        class_index = int(parts[0])\n",
    "\n",
    "        # Get the class name\n",
    "        class_name = class_names[class_index]\n",
    "\n",
    "        # Increment the count for the class\n",
    "        class_counts[class_name] += 1\n",
    "\n",
    "# Determine the minimum number of samples in any class\n",
    "# min_samples = min(class_counts.values())\n",
    "data_median = np.median([val for val in class_counts.values() if val>0])\n",
    "p_under_represented = lambda x: abs(min(0,(class_counts[class_names[x]]-data_median))/data_median)\n",
    "p_over_represented  = 0.3\n",
    "\n",
    "\n",
    "min_samples = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to store the augmented annotations\n",
    "augmented_annotations = [] \n",
    "\n",
    "no_color_aug_classes = [8, 5, ] # no color aug\n",
    "\n",
    "\n",
    "# transform 1 : no color changes\n",
    "transform1 = A.Compose([\n",
    "                        A.HorizontalFlip(p=0.5),\n",
    "                        A.ShiftScaleRotate(p=0.5),\n",
    "                        A.CLAHE(p=0.5),\n",
    "                        A.Emboss(p=0.2),\n",
    "                        A.PiecewiseAffine(scale=(0.01, 0.03), p=0.3),\n",
    "                        A.Perspective(p=0.3),\n",
    "                        A.RandomBrightness(p=0.25),\n",
    "                        A.Cutout(p=0.15, num_holes=15, max_h_size=30, max_w_size=20, fill_value=0),\n",
    "                    ],\n",
    "                    bbox_params=A.BboxParams(format='coco', label_fields=['class_annotations']),\n",
    "                )\n",
    "# tranform 2 : with color changes\n",
    "transform2 = A.Compose([\n",
    "                        A.HorizontalFlip(p=0.5),\n",
    "                        A.ShiftScaleRotate(p=0.5),\n",
    "                        A.RandomBrightnessContrast(p=0.3),\n",
    "                        A.RGBShift(r_shift_limit=30, g_shift_limit=30, b_shift_limit=30, p=0.3),\n",
    "                        A.CLAHE(p=0.5),\n",
    "                        A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=25, val_shift_limit=30, p=0.2),\n",
    "                        A.Emboss(p=0.2),\n",
    "                        A.PiecewiseAffine(scale=(0.01, 0.03), p=0.3),\n",
    "                        A.Perspective(p=0.3),\n",
    "                        A.RandomBrightness(p=0.25),\n",
    "                        A.RandomContrast(p=0.25),\n",
    "                        A.RandomFog(p=0.05, fog_coef_lower=0.01, fog_coef_upper=0.05, alpha_coef=0.08),\n",
    "                        A.ToGray(p=0.1),\n",
    "                        A.Cutout(p=0.15, num_holes=15, max_h_size=30, max_w_size=20, fill_value=0),\n",
    "                    ],\n",
    "                    bbox_params=A.BboxParams(format='coco', label_fields=['class_annotations']),\n",
    "                )\n",
    "\n",
    "          \n",
    "# Iterate over all of the images\n",
    "for image_file in os.listdir(images_path):\n",
    "    # Load the image\n",
    "    image = cv2.imread(os.path.join(images_path, image_file))\n",
    "    img_h = image.shape[0]\n",
    "    img_w = image.shape[1]\n",
    "\n",
    "\n",
    "    # Get the corresponding annotation file\n",
    "    annotation_file = image_file.replace(\".jpg\", \".txt\")\n",
    "    annotation_file = os.path.join(annotations_path, annotation_file)\n",
    "\n",
    "    # Open the annotation file\n",
    "    with open(annotation_file, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Iterate over each line of the annotation file\n",
    "    image_annotations = []\n",
    "    class_annotations = []\n",
    "    for line in lines:\n",
    "        # Split the line into parts\n",
    "        parts = line.strip().split()\n",
    "\n",
    "        # Get the class index\n",
    "        class_index = int(parts[0])\n",
    "\n",
    "        # Get the class name\n",
    "        class_name = class_names[class_index]\n",
    "\n",
    "        # class x_center y_center width height\n",
    "\n",
    "        bx_w    = int(float(parts[3])*img_w)\n",
    "        bx_h    = int(float(parts[4])*img_h)\n",
    "        bx_xmin = max(0,min(img_w,int((float(parts[1])- (float(parts[3])/2))*img_w)))\n",
    "        bx_ymin = max(0,min(img_h,int((float(parts[2])- (float(parts[4])/2))*img_h)))\n",
    "        bx_xmax = bx_xmin+bx_w\n",
    "        bx_ymax = bx_ymin+bx_h\n",
    "\n",
    "        image_annotations.append([bx_xmin, bx_ymin, bx_w, bx_h ])\n",
    "        # image_annotations.append([class_index, bx_xmin, bx_ymin, bx_xmax, bx_ymax ])\n",
    "        # image_annotations.append([bx_xmin, bx_ymin, bx_xmax, bx_ymax ])\n",
    "        class_annotations.append(class_index)\n",
    "\n",
    "        # If the class is underrepresented\n",
    "\n",
    "        aug_probability = max(p_under_represented(class_index), p_over_represented)\n",
    "        if random.random() < aug_probability:\n",
    "            # Determine the number of samples to add\n",
    "            # samples_to_add = min_samples - class_counts[class_name]\n",
    "            samples_to_add = get_num_samples(aug_probability)\n",
    "\n",
    "            #### New Aug\n",
    "            # Augment the image\n",
    "            try:\n",
    "                for i in range(samples_to_add):\n",
    "                    category_ids = [c[0] for c in image_annotations]\n",
    "                    if class_index in no_color_aug_classes:\n",
    "                        transformed = transform1(image=image, bboxes=image_annotations, class_annotations=class_annotations)\n",
    "                    else:\n",
    "                        transformed = transform2(image=image, bboxes=image_annotations, class_annotations=class_annotations)\n",
    "\n",
    "                    augmented_image = transformed['image']\n",
    "                    augmented_annotations = transformed['bboxes']\n",
    "                    class_annotations = transformed['class_annotations']\n",
    "\n",
    "                    # augmented_image = seq(images=np.array([image]))[0]\n",
    "                    cv2.imwrite(os.path.join(aug_images_path, f\"{image_file.split('.')[0]}_{i}.jpg\"), augmented_image)\n",
    "\n",
    "                    augmented_annotations = convert_bbox_coco2yolo(img_width=img_w, img_height=img_h, bbox=augmented_annotations, cls_=class_annotations)\n",
    "\n",
    "                    # Write the augmented annotations to a new file\n",
    "                    with open(os.path.join(aug_annotations_path, f\"{image_file.split('.')[0]}_{i}.txt\"), \"w\") as f:\n",
    "                        for annotation in augmented_annotations:\n",
    "                            f.write(\" \".join([str(x) for x in annotation]) + \"\\n\")\n",
    "            except:\n",
    "                pass\n",
    "            #### End New Aug\n",
    "\n",
    "\n",
    "# Now you can use the \"augmented_annotations.txt\" file to train your YOLOv5or7or8 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directories containing images and annotations\n",
    "image_dir = \"train/images\"\n",
    "annotated_images = image_dir\n",
    "annotation_dir = \"train/labels\"\n",
    "\n",
    "\n",
    "# Loop through all images in the image directory\n",
    "for image_file in os.listdir(image_dir):\n",
    "    # Check if the file is a JPG image\n",
    "    if image_file.endswith('.jpg'):\n",
    "        # Load the image\n",
    "        image = cv2.imread(os.path.join(image_dir, image_file))\n",
    "        # Get the image width and height\n",
    "        img_height, img_width, _ = image.shape\n",
    "        # Load the corresponding annotation file\n",
    "        annotation_file = image_file.replace('.jpg', '.txt')\n",
    "        with open(os.path.join(annotation_dir, annotation_file)) as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        # Loop through all lines in the annotation file\n",
    "        for line in lines:\n",
    "            # Extract the annotation values\n",
    "            class_id, x_center, y_center, width, height = line.strip().split()\n",
    "            # Convert the annotation values to integers\n",
    "            x_center = int(float(x_center) *float(img_width))\n",
    "            y_center = int(float(y_center) * float(img_height))\n",
    "            width = int(float(width) * float(img_width))\n",
    "            height = int(float(height) * float(img_height))\n",
    "\n",
    "            # calculate the xmin, ymin, xmax, ymax\n",
    "            xmin = int(x_center - (width / 2))\n",
    "            ymin = int(y_center - (height / 2))\n",
    "            xmax = int(x_center + (width / 2))\n",
    "            ymax = int(y_center + (height / 2))\n",
    "            # Draw the boundary box on the image\n",
    "            cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n",
    "        \n",
    "        # Save the image with the boundary boxes plotted\n",
    "        output_file = image_file.replace('.jpg', '_annotated.jpg')\n",
    "        cv2.imwrite(os.path.join(annotated_images, output_file), image)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ZY2VXXXu74w5"
   },
   "source": [
    "# 2. Train\n",
    "\n",
    "<p align=\"\"><a><img width=\"1000\" src=\"https://i.imgur.com/PpaBsmW.jpg\"/></a></p>\n",
    "\n",
    "Train the model on detecting the labeled images, with the following tuned arguments, the rest is kept as the default:\n",
    "\n",
    "| Argument                                                             | Value          | \n",
    "|----------------------------------------------------------------------------|--------------------|\n",
    "| `task`                                            | detect                           \n",
    "| `mode`                    | train      \n",
    "| `model`                                                   | yolov8x.pt                  \n",
    "| `data`                    | data.yaml        \n",
    "|`epochs`                          | 60           \n",
    "| `patience`                             | 10          \n",
    "| `batch`      | 8      |     |\n",
    "| `imgsz` | 640               \n",
    "| `device`                         | 0           \n",
    "| `workers`         |8          \n",
    "|`optimizer`                             | SGD           \n",
    "|`augment`                            | true       \n",
    "|`iou`                            | 0.7         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CYIjW4igCjqD",
    "outputId": "3bb45917-f90e-4951-959d-7bcd26680f2e"
   },
   "outputs": [],
   "source": [
    "!yolo task=detect mode=train model=yolov8x.pt  epochs=60 data=data.yaml workers=8 imgsz=640 batch=8  device=0 augment=true patience=10 dropout=0.05"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Phm9ccmOKye5"
   },
   "source": [
    "# 3. Prediction\n",
    "<img width=\"1024\" src=\"https://i.imgur.com/VwgnWek.jpg\">\n",
    "\n",
    "This section is for validation, testing, and postprocessing\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "yq26lwpYK1lq"
   },
   "source": [
    "## 3.1 Validation\n",
    "\n",
    "Validate a model's performance on the 1st theme dataset's `val` or `test` splits (90% `training`, 10% `validation`). The best custom-trained model is used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best weight and validate it using the dataset  \n",
    "!yolo task=detect mode=val model=best.pt data=data.yaml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "7ZW58jUzK66B"
   },
   "source": [
    "## 3.2 Model Postprocessing\n",
    "\n",
    "Once the model is trained, there are some classes that are under presented. Therefore, I have proposed and used a technique called \"class-based acceptance\" and \"Class-based bounding box\" to tune the model's confidence based on the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WFPJIQl_L5HT"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Class-based acceptance : A method to accept a boundry box based on \n",
    "the confidence of the prediction, note that for each class we have  \n",
    "a confidence level that will assure us a precision of 60% or higher\n",
    "'''\n",
    "\n",
    "def accept_bbox(bbox: list) -> bool:\n",
    "    # bbox = [cls_ xmax xmin ymax ymin conf]\n",
    "    # class2conf is a dictionary of acceptance confidence values for each class\n",
    "    class2conf = {  0:0.2, 1:0.35, 2:0.4, 3:0.3, 4:0.2, 5:0.3, \n",
    "                    6:0.1, 7:0.25, 8:0.35, 9:0.18, 10:0.45} # 0.5515\n",
    "\n",
    "    # class2conf = {  k:v*2 for k,v in class2conf.items()} # double harsh [score = 0.4325]\n",
    "    class2conf = {  k:v/2 for k,v in class2conf.items()} # double soft [0.5518]\n",
    "    # class2conf = {  k:v/4 for k,v in class2conf.items()}   # quad soft [0.0]\n",
    "\n",
    "    # class2conf = {i:0.1 for i in range(11)} # 0.5518\n",
    "\n",
    "    if float(bbox[-1]) >= class2conf[int(bbox[0])]:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_similar_boxes(bboxes: list) -> list:\n",
    "    return bboxes\n",
    "    # hyperparameters:\n",
    "    # boxes that intersect by intersection_thresh % of their total area are counted as similar, and are merged\n",
    "    original_bboxes = bboxes[:]\n",
    "    intersection_thresh = 0.7\n",
    "    add_both = False\n",
    "    # the custom probability of merging similar boxes for some certain classes\n",
    "    class2merge_acceptance_proba = {0:0.3 , 3:0.2, 5:0.3 ,7:0.2 } \n",
    "    default_merge_acceptance_proba = 0.3 # probability of merging similar boxes\n",
    "    # bboxes = [[cls_ xmax xmin ymax ymin conf], ...]\n",
    "    # bboxes = [[current_cls, img, class_names[current_cls], xmax//2, xmin//2, ymax//2, ymin//2, conf], ...]\n",
    "    merged_groupes = {}\n",
    "    num_boxes_merged = 0\n",
    "    filtered_boxes = []\n",
    "    for i,first_box in enumerate(bboxes[:-1]):\n",
    "        my_parent = \"No body\"\n",
    "        has_intersection = False\n",
    "        best_box = first_box[:]\n",
    "        for current_box in bboxes[i+1:]:\n",
    "            cls_1, _ , _ , xmax1, xmin1, ymax1, ymin1, conf1 = first_box\n",
    "            cls_2, _ , _ , xmax2, xmin2, ymax2, ymin2, conf2 = current_box\n",
    "            box1 = [xmax1, xmin1, ymax1, ymin1, conf1]\n",
    "            box2 = [xmax2, xmin2, ymax2, ymin2, conf2]\n",
    "            box_intersection = intersection_area(box1, box2)\n",
    "            same_classes = (cls_1==cls_2)\n",
    "            enough_intersection = False\n",
    "            accept_intersection = True\n",
    "\n",
    "            if box_intersection > intersection_thresh:\n",
    "                enough_intersection = True\n",
    "            \n",
    "            # if all merge conditions are met:\n",
    "            if same_classes and enough_intersection:\n",
    "                has_intersection=True\n",
    "                if first_box[-1]>current_box[-1]:\n",
    "                    best_box = first_box[:]\n",
    "                else:\n",
    "                    best_box = current_box[:]\n",
    "                \n",
    "        if not has_intersection:\n",
    "            if add_both:\n",
    "                filtered_boxes.append(first_box[:-1])\n",
    "                filtered_boxes.append(current_box[:-1])\n",
    "            else:\n",
    "                filtered_boxes.append(first_box[:-1])\n",
    "        else:\n",
    "            filtered_boxes.append(best_box[:-1])\n",
    "    \n",
    "    if len(filtered_boxes) > 0:\n",
    "        return filtered_boxes\n",
    "    \n",
    "    return [box[:-1] for box in original_bboxes[:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection_area(box1, box2):\n",
    "    # unpack the coordinates of the rectangles\n",
    "    xmax1, xmin1, ymax1, ymin1,_ = box1\n",
    "    xmax2, xmin2, ymax2, ymin2,_ = box2\n",
    "\n",
    "    # calculate the intersection area\n",
    "    x_intersection = max(0, min(xmax1, xmax2) - max(xmin1, xmin2))\n",
    "    y_intersection = max(0, min(ymax1, ymax2) - max(ymin1, ymin2))\n",
    "    intersection = x_intersection * y_intersection\n",
    "\n",
    "    # calculate the total area of the rectangles\n",
    "    total_area = (xmax1 - xmin1) * (ymax1 - ymin1) + (xmax2 - xmin2) * (ymax2 - ymin2) - intersection\n",
    "\n",
    "    # calculate the normalized intersection area\n",
    "    if total_area <=0:\n",
    "        normalized_intersection = 0\n",
    "    else:\n",
    "        normalized_intersection = intersection / total_area\n",
    "\n",
    "    return normalized_intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths for the images and annotations\n",
    "images_path = \"images\"\n",
    "model_paths = [ \"best.pt\"]\n",
    "test_csv_path = \"test.csv\"\n",
    "\n",
    "\n",
    "class_names = {\n",
    "    0:\"GRAFFITI\",\n",
    "    1:\"FADED_SIGNAGE\",\n",
    "    2:\"POTHOLES\",\n",
    "    3:\"GARBAGE\",\n",
    "    4:\"CONSTRUCTION_ROAD\",\n",
    "    5:\"BROKEN_SIGNAGE\",\n",
    "    6:\"BAD_STREETLIGHT\",\n",
    "    7:\"BAD_BILLBOARD\",\n",
    "    8:\"SAND_ON_ROAD\",\n",
    "    9:\"CLUTTER_SIDEWALK\",\n",
    "    10:\"UNKEPT_FACADE\"\n",
    "}\n",
    "\n",
    "category_ids = class_names.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best trained model\n",
    "models = [YOLO(p) for p in model_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names = []\n",
    "with open(test_csv_path, 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    next(reader) # skip the header row\n",
    "    for row in reader:\n",
    "        image_names.append(row[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Phm9ccmOKye5"
   },
   "source": [
    "# 4. Results\n",
    "<img width=\"1024\" src=\"https://i.imgur.com/Dv4lkAg.jpg\">\n",
    "\n",
    "This section contains the results of the trained model with different examples, various evaluation metrics\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "<img align=\"left\" src=\"https://i.imgur.com/D1S3oD8.png\" width=\"600\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "<img align=\"left\" src=\"https://i.imgur.com/yDAEPX6.png\" width=\"600\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "<img align=\"left\" src=\"https://i.imgur.com/28OhYXb.png\" width=\"600\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "<img align=\"left\" src=\"https://i.imgur.com/FmHSuoa.jpg\" width=\"600\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "<img align=\"left\" src=\"https://i.imgur.com/TCZgcec.jpg\" width=\"600\">"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "YOLOv8 Tutorial",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "6861ee0e72a6aa5d06047cac410bdc16f6f1e5a21e56d02c16f890483dc12add"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
